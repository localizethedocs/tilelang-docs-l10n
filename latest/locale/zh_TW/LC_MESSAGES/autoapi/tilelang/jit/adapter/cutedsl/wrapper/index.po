# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025-2025, Tile Lang Contributors
# This file is distributed under the same license as the TileLang <br> package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TileLang <br> latest\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-29 08:24+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:2
msgid "tilelang.jit.adapter.cutedsl.wrapper"
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:8
msgid "CuTeDSL Source Wrapper for TileLang."
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:10
msgid ""
"This module provides C++ kernel launcher generation for the CuTeDSL backend."
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:12
msgid ""
"Key features: - Automatic C++ launcher generation with CUDA Driver API - TMA "
"descriptors on HOST memory, passed via __grid_constant__ (no device copy "
"needed) - cuLaunchKernel automatically copies 128-byte CUtensorMap to kernel "
"param space - Support for single and multiple kernel launches - Complete "
"cache system integration"
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:22
msgid "Attributes"
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:39:<autosummary>:1
msgid ""
":py:obj:`CPP_TMA_DESC_INIT_TEMPLATE <tilelang.jit.adapter.cutedsl.wrapper."
"CPP_TMA_DESC_INIT_TEMPLATE>`\\"
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:39:<autosummary>:1
msgid ""
":py:obj:`CPP_TMA_IM2COL_DESC_INIT_TEMPLATE <tilelang.jit.adapter.cutedsl."
"wrapper.CPP_TMA_IM2COL_DESC_INIT_TEMPLATE>`\\"
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:39:<autosummary>:1
msgid ""
":py:obj:`CPP_TMA_INIT_FUNC_TEMPLATE <tilelang.jit.adapter.cutedsl.wrapper."
"CPP_TMA_INIT_FUNC_TEMPLATE>`\\"
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:39:<autosummary>:1
msgid ""
":py:obj:`CPP_KERNEL_INIT_TEMPLATE <tilelang.jit.adapter.cutedsl.wrapper."
"CPP_KERNEL_INIT_TEMPLATE>`\\"
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:39:<autosummary>:1
msgid ""
":py:obj:`CPP_TMA_LAUNCH_INIT_TEMPLATE <tilelang.jit.adapter.cutedsl.wrapper."
"CPP_TMA_LAUNCH_INIT_TEMPLATE>`\\"
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:39:<autosummary>:1
msgid ""
":py:obj:`CPP_KERNEL_LAUNCH_TEMPLATE <tilelang.jit.adapter.cutedsl.wrapper."
"CPP_KERNEL_LAUNCH_TEMPLATE>`\\"
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:39:<autosummary>:1
msgid ""
":py:obj:`CPP_LAUNCHER_TEMPLATE <tilelang.jit.adapter.cutedsl.wrapper."
"CPP_LAUNCHER_TEMPLATE>`\\"
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:39:<autosummary>:1
msgid ""
":py:obj:`CUBIN_TMA_ATOM_INIT_TEMPLATE <tilelang.jit.adapter.cutedsl.wrapper."
"CUBIN_TMA_ATOM_INIT_TEMPLATE>`\\"
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:39:<autosummary>:1
msgid ""
":py:obj:`CUBIN_KERNEL_LAUNCH_TEMPLATE <tilelang.jit.adapter.cutedsl.wrapper."
"CUBIN_KERNEL_LAUNCH_TEMPLATE>`\\"
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:39:<autosummary>:1
msgid ""
":py:obj:`CUBIN_FAKE_TENSOR_TEMPLATE <tilelang.jit.adapter.cutedsl.wrapper."
"CUBIN_FAKE_TENSOR_TEMPLATE>`\\"
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:39:<autosummary>:1
msgid ""
":py:obj:`CUBIN_GEN_CODE_TEMPLATE <tilelang.jit.adapter.cutedsl.wrapper."
"CUBIN_GEN_CODE_TEMPLATE>`\\"
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:39:<autosummary>:1
msgid ""
":py:obj:`PYTHON_HOST_FUNC_TEMPLATE <tilelang.jit.adapter.cutedsl.wrapper."
"PYTHON_HOST_FUNC_TEMPLATE>`\\"
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:41
msgid "Classes"
msgstr "類別"

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:47:<autosummary>:1
msgid ""
":py:obj:`TLCuTeDSLSourceWrapper <tilelang.jit.adapter.cutedsl.wrapper."
"TLCuTeDSLSourceWrapper>`\\"
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:780
#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:47:<autosummary>:1
msgid "Wrapper class for TileLang CuTe DSL backend with C++ launcher."
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:49
msgid "Module Contents"
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:54
#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:98
#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:146
#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:172
#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:205
#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:234
#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:276
#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:594
#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:620
#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:667
msgid "<details><summary>Show Value</summary>"
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:58
msgid ""
"\"\"\"  // Descriptor {desc_idx}: {desc_name} (tensor: {tensor_name})\n"
"  {{\n"
"    uint64_t globalDim[{rank}] = {{{global_dim_values}}};\n"
"    uint64_t globalStrides[{stride_rank}] = {{{global_stride_values}}};\n"
"    uint32_t boxDim[{rank}] = {{{box_dim_values}}};\n"
"    uint32_t elemStrides[{rank}] = {{{elem_stride_values}}};\n"
"\n"
"    result = cuTensorMapEncodeTiled(\n"
"        &tma_descs[{desc_idx}],\n"
"        static_cast<CUtensorMapDataType>({dtype}),\n"
"        {rank},\n"
"        reinterpret_cast<void*>({tensor_name}_ptr),\n"
"        globalDim,\n"
"        globalStrides,\n"
"        boxDim,\n"
"        elemStrides,\n"
"        static_cast<CUtensorMapInterleave>({interleave}),\n"
"        static_cast<CUtensorMapSwizzle>({swizzle}),\n"
"        static_cast<CUtensorMapL2promotion>({l2_promotion}),\n"
"        static_cast<CUtensorMapFloatOOBfill>({oob_fill})\n"
"    );\n"
"\n"
"    if (result != CUDA_SUCCESS) {{\n"
"      std::cerr << \"Failed to encode TMA descriptor {desc_idx}: \" << "
"result << \"\\n\";\n"
"      return result;\n"
"    }}\n"
"  }}\n"
"\"\"\""
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:89
#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:137
#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:163
#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:196
#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:225
#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:267
#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:581
#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:607
#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:658
#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:769
msgid "</details>"
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:102
msgid ""
"\"\"\"  // Descriptor {desc_idx}: {desc_name} (tensor: {tensor_name}) "
"[im2col]\n"
"  {{\n"
"    uint64_t globalDim[{rank}] = {{{global_dim_values}}};\n"
"    uint64_t globalStrides[{stride_rank}] = {{{global_stride_values}}};\n"
"    uint32_t elemStrides[{rank}] = {{{elem_stride_values}}};\n"
"    int32_t lowerCorner[{rank_minus_two}] = {{{lower_corner_values}}};\n"
"    int32_t upperCorner[{rank_minus_two}] = {{{upper_corner_values}}};\n"
"\n"
"    result = cuTensorMapEncodeIm2col(\n"
"        &tma_descs[{desc_idx}],\n"
"        static_cast<CUtensorMapDataType>({dtype}),\n"
"        {rank},\n"
"        reinterpret_cast<void*>({tensor_name}_ptr),\n"
"        globalDim,\n"
"        globalStrides,\n"
"        lowerCorner,\n"
"        upperCorner,\n"
"        static_cast<uint32_t>({channels_per_pixel}),\n"
"        static_cast<uint32_t>({pixels_per_column}),\n"
"        elemStrides,\n"
"        static_cast<CUtensorMapInterleave>({interleave}),\n"
"        static_cast<CUtensorMapSwizzle>({swizzle}),\n"
"        static_cast<CUtensorMapL2promotion>({l2_promotion}),\n"
"        static_cast<CUtensorMapFloatOOBfill>({oob_fill})\n"
"    );\n"
"\n"
"    if (result != CUDA_SUCCESS) {{\n"
"      std::cerr << \"Failed to encode TMA im2col descriptor {desc_idx}: \" "
"<< result << \"\\n\";\n"
"      return result;\n"
"    }}\n"
"  }}\n"
"\"\"\""
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:150
msgid ""
"\"\"\"CUresult tma_init(CUtensorMap* tma_descs, {func_args}) {{\n"
"  // Initialize {num_descs} TMA descriptor(s) in caller-provided host array\n"
"  // cuLaunchKernel will copy 128-byte CUtensorMap to kernel param space "
"automatically\n"
"  CUresult result;\n"
"\n"
"{desc_init_code}\n"
"\n"
"  return CUDA_SUCCESS;\n"
"}}\n"
"\"\"\""
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:176
msgid ""
"\"\"\"  // Find and configure kernel {kernel_idx}: {kernel_name}\n"
"  result = find_kernel_by_pattern(module, \"{kernel_name}\", "
"&kernels[{kernel_idx}]);\n"
"  if (result != CUDA_SUCCESS) {{\n"
"    std::cerr << \"Failed to find kernel {kernel_name} on device \" << "
"device_id << \": \" << result << \"\\n\";\n"
"    return result;\n"
"  }}\n"
"\n"
"  if ({smem_size} > 0) {{\n"
"    result = cuFuncSetAttribute(kernels[{kernel_idx}],\n"
"                                CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES,\n"
"                                {smem_size});\n"
"    if (result != CUDA_SUCCESS) {{\n"
"      std::cerr << \"Failed to set smem for {kernel_name} on device \" << "
"device_id << \": \" << result << \"\\n\";\n"
"      return result;\n"
"    }}\n"
"  }}\n"
"\"\"\""
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:209
msgid ""
"\"\"\"  // Declare stack-local TMA descriptor array (eliminates concurrency "
"race)\n"
"  CUtensorMap tma_descs[{num_tma_descs}];\n"
"\n"
"  // Initialize TMA descriptors (HOST memory - passed via "
"__grid_constant__)\n"
"  // NOTE: We intentionally do NOT reuse/cached descriptors across "
"launches.\n"
"  // Pointer-only reuse is a correctness trap (shape/stride may change with "
"same ptr),\n"
"  // and correctness beats micro-optimizations.\n"
"  result = tma_init(tma_descs, {tma_tensor_args});\n"
"  if (result != CUDA_SUCCESS) {{\n"
"    std::cerr << \"Failed to initialize TMA descriptors: \" << result << "
"\"\\n\";\n"
"    return result;\n"
"  }}\n"
"\"\"\""
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:238
msgid ""
"\"\"\"  // Launch kernel {kernel_idx}: {kernel_name}\n"
"  {{\n"
"    // Get the kernel for current device\n"
"    auto kernels_it = g_device_kernels.find(device_id);\n"
"    if (kernels_it == g_device_kernels.end()) {{\n"
"      std::cerr << \"Kernels not initialized for device \" << device_id << "
"\"\\n\";\n"
"      return CUDA_ERROR_NOT_INITIALIZED;\n"
"    }}\n"
"    const std::vector<CUfunction>& kernels = kernels_it->second;\n"
"\n"
"    void* args[] = {{{kernel_args}}};\n"
"    result = cuLaunchKernel(\n"
"        kernels[{kernel_idx}],\n"
"        {grid_x}, {grid_y}, {grid_z},\n"
"        {block_x}, {block_y}, {block_z},\n"
"        {smem_size},\n"
"        stream,\n"
"        args,\n"
"        nullptr\n"
"    );\n"
"    if (result != CUDA_SUCCESS) {{\n"
"      std::cerr << \"Failed to launch kernel {kernel_name} on device \" << "
"device_id << \": \" << result << \"\\n\";\n"
"      return result;\n"
"    }}\n"
"  }}\n"
"\"\"\""
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:280
msgid ""
"\"\"\"#include <cuda.h>\n"
"#include <cstdint>\n"
"#include <iostream>\n"
"#include <fstream>\n"
"#include <vector>\n"
"#include <cstring>\n"
"#include <string>\n"
"#include <mutex>\n"
"#include <unordered_map>\n"
"\n"
"// TVM Headers\n"
"#include <tvm/ffi/container/tensor.h>\n"
"#include <tvm/ffi/extra/c_env_api.h>\n"
"#include <tvm/ffi/function.h>\n"
"\n"
"// Per-device module and kernel storage for multi-GPU support\n"
"// Each device needs its own CUmodule because modules are tied to CUDA "
"contexts\n"
"static std::unordered_map<int, CUmodule> g_device_modules;\n"
"static std::unordered_map<int, std::vector<CUfunction>> g_device_kernels;\n"
"static std::unordered_map<int, CUcontext> g_device_contexts;  // Track "
"retained contexts for cleanup\n"
"static std::mutex g_devices_mutex;\n"
"\n"
"// Find kernel by pattern (substring match, prefer base name over _N "
"variants)\n"
"CUresult find_kernel_by_pattern(CUmodule module, const char* pattern, "
"CUfunction* out_func) {{\n"
"  CUresult result;\n"
"  unsigned int num_funcs = 0;\n"
"\n"
"  result = cuModuleGetFunctionCount(&num_funcs, module);\n"
"  if (result != CUDA_SUCCESS) {{\n"
"    std::cerr << \"Failed to get function count: \" << result << \"\\n\";\n"
"    return result;\n"
"  }}\n"
"\n"
"  std::vector<CUfunction> func_list(num_funcs);\n"
"  result = cuModuleEnumerateFunctions(func_list.data(), num_funcs, module);\n"
"  if (result != CUDA_SUCCESS) {{\n"
"    std::cerr << \"Failed to enumerate functions: \" << result << \"\\n\";\n"
"    return result;\n"
"  }}\n"
"\n"
"  // Collect substring matches, separating base name from _N variants\n"
"  std::vector<std::pair<std::string, CUfunction>> base_matches;     // "
"pattern not followed by _digit\n"
"  std::vector<std::pair<std::string, CUfunction>> variant_matches;  // "
"pattern followed by _digit\n"
"\n"
"  size_t pattern_len = std::strlen(pattern);\n"
"\n"
"  for (unsigned int i = 0; i < num_funcs; i++) {{\n"
"    const char* func_name = nullptr;\n"
"    result = cuFuncGetName(&func_name, func_list[i]);\n"
"    if (result != CUDA_SUCCESS || func_name == nullptr) {{\n"
"      std::cerr << \"Failed to get function name: \" << result << \"\\n\";\n"
"      return result;\n"
"    }}\n"
"\n"
"    std::string name_str(func_name);\n"
"    size_t pos = name_str.find(pattern);\n"
"\n"
"    if (pos != std::string::npos) {{\n"
"      // Found substring match\n"
"      size_t after_pattern = pos + pattern_len;\n"
"\n"
"      // Check what follows the pattern\n"
"      if (after_pattern < name_str.length() &&\n"
"          name_str[after_pattern] == '_' &&\n"
"          after_pattern + 1 < name_str.length() &&\n"
"          std::isdigit(name_str[after_pattern + 1])) {{\n"
"        // Pattern followed by _digit (e.g., \"main_kernel_1\")\n"
"        variant_matches.push_back({{name_str, func_list[i]}});\n"
"      }} else {{\n"
"        // Pattern not followed by _digit (e.g., \"main_kernel\" itself)\n"
"        base_matches.push_back({{name_str, func_list[i]}});\n"
"      }}\n"
"    }}\n"
"  }}\n"
"\n"
"  // Decision logic: prefer base matches over variant matches\n"
"  if (!base_matches.empty()) {{\n"
"    if (base_matches.size() == 1) {{\n"
"      *out_func = base_matches[0].second;\n"
"      return CUDA_SUCCESS;\n"
"    }}\n"
"\n"
"    // Multiple base matches - ambiguous\n"
"    std::cerr << \"Error: Pattern '\" << pattern << \"' matched \" << "
"base_matches.size()\n"
"              << \" base kernels (ambiguous). Matches found:\\n\";\n"
"    for (const auto& match : base_matches) {{\n"
"      std::cerr << \"  - \" << match.first << \"\\n\";\n"
"    }}\n"
"    std::cerr << \"Please use a more specific pattern.\\n\";\n"
"    return CUDA_ERROR_NOT_FOUND;\n"
"  }}\n"
"\n"
"  // No base matches, try variant matches\n"
"  if (!variant_matches.empty()) {{\n"
"    if (variant_matches.size() == 1) {{\n"
"      *out_func = variant_matches[0].second;\n"
"      return CUDA_SUCCESS;\n"
"    }}\n"
"\n"
"    // Multiple variant matches - ambiguous\n"
"    std::cerr << \"Error: Pattern '\" << pattern << \"' matched \" << "
"variant_matches.size()\n"
"              << \" variant kernels (ambiguous). Matches found:\\n\";\n"
"    for (const auto& match : variant_matches) {{\n"
"      std::cerr << \"  - \" << match.first << \"\\n\";\n"
"    }}\n"
"    std::cerr << \"Please use a more specific pattern (e.g., '\" << pattern "
"<< \"_1').\\n\";\n"
"    return CUDA_ERROR_NOT_FOUND;\n"
"  }}\n"
"\n"
"  // No matches at all\n"
"  std::cerr << \"Failed to find kernel matching pattern '\" << pattern << "
"\"'\\n\";\n"
"  return CUDA_ERROR_NOT_FOUND;\n"
"}}\n"
"\n"
"\n"
"// Initialize CUDA module for a specific device (called once per device)\n"
"// Thread-safe and supports multi-GPU by tracking modules per device\n"
"// device_id: PyTorch CUDA device ID (e.g., 0, 1, 2...)\n"
"static CUresult tilelang_init_cuda_module(const std::string& cubin_path, int "
"device_id) {{\n"
"  std::lock_guard<std::mutex> lock(g_devices_mutex);\n"
"\n"
"  // Fast path: module already initialized for this device\n"
"  if (g_device_modules.find(device_id) != g_device_modules.end()) {{\n"
"    return CUDA_SUCCESS;\n"
"  }}\n"
"\n"
"  CUresult result;\n"
"  result = cuInit(0);\n"
"  if (result != CUDA_SUCCESS) {{\n"
"    std::cerr << \"Failed to initialize CUDA: \" << result << \"\\n\";\n"
"    return result;\n"
"  }}\n"
"\n"
"  // Get device handle for the requested device_id\n"
"  CUdevice device;\n"
"  result = cuDeviceGet(&device, device_id);\n"
"  if (result != CUDA_SUCCESS) {{\n"
"    std::cerr << \"Failed to get CUDA device \" << device_id << \": \" << "
"result << \"\\n\";\n"
"    return result;\n"
"  }}\n"
"\n"
"  // Retain and set the primary context for this device\n"
"  // PyTorch (Runtime API) creates and activates the primary context\n"
"  // We need to explicitly acquire it via Driver API and set it as current\n"
"  CUcontext ctx;\n"
"  result = cuDevicePrimaryCtxRetain(&ctx, device);\n"
"  if (result != CUDA_SUCCESS) {{\n"
"    std::cerr << \"Failed to retain primary context for device \" << "
"device_id << \": \" << result << \"\\n\";\n"
"    return result;\n"
"  }}\n"
"\n"
"  result = cuCtxSetCurrent(ctx);\n"
"  if (result != CUDA_SUCCESS) {{\n"
"    std::cerr << \"Failed to set current context for device \" << device_id "
"<< \": \" << result << \"\\n\";\n"
"    return result;\n"
"  }}\n"
"\n"
"  // Store the retained context for cleanup\n"
"  g_device_contexts[device_id] = ctx;\n"
"\n"
"  // Read cubin file\n"
"  std::ifstream cubin_file(cubin_path.c_str(), std::ios::binary);\n"
"  if (!cubin_file) {{\n"
"    std::cerr << \"Failed to open cubin file: \" << cubin_path << \"\\n\";\n"
"    return CUDA_ERROR_FILE_NOT_FOUND;\n"
"  }}\n"
"\n"
"  std::vector<char> cubin_data((std::"
"istreambuf_iterator<char>(cubin_file)),\n"
"                                std::istreambuf_iterator<char>());\n"
"  cubin_file.close();\n"
"\n"
"  if (cubin_data.empty()) {{\n"
"    std::cerr << \"Empty cubin file: \" << cubin_path << \"\\n\";\n"
"    return CUDA_ERROR_INVALID_IMAGE;\n"
"  }}\n"
"\n"
"  // Load module for this specific device\n"
"  CUmodule module;\n"
"  result = cuModuleLoadData(&module, cubin_data.data());\n"
"  if (result != CUDA_SUCCESS) {{\n"
"    std::cerr << \"Failed to load CUDA module on device \" << device_id << "
"\": \" << result << \"\\n\";\n"
"    return result;\n"
"  }}\n"
"\n"
"  // Store module for this device\n"
"  g_device_modules[device_id] = module;\n"
"\n"
"  return CUDA_SUCCESS;\n"
"}}\n"
"\n"
"// Initialize kernel functions for a specific device (called once per "
"device)\n"
"// Thread-safe and supports multi-GPU by tracking kernels per device\n"
"static CUresult tilelang_init_kernels(int device_id) {{\n"
"  std::lock_guard<std::mutex> lock(g_devices_mutex);\n"
"\n"
"  // Fast path: kernels already initialized for this device\n"
"  if (g_device_kernels.find(device_id) != g_device_kernels.end()) {{\n"
"    return CUDA_SUCCESS;\n"
"  }}\n"
"\n"
"  // Get the module for this device\n"
"  auto module_it = g_device_modules.find(device_id);\n"
"  if (module_it == g_device_modules.end()) {{\n"
"    std::cerr << \"Module not initialized for device \" << device_id << "
"\"\\n\";\n"
"    return CUDA_ERROR_NOT_INITIALIZED;\n"
"  }}\n"
"  CUmodule module = module_it->second;\n"
"\n"
"  // Initialize kernel storage for this device\n"
"  std::vector<CUfunction> kernels({num_kernels});\n"
"  CUresult result;\n"
"\n"
"{kernel_inits}\n"
"\n"
"  // Store kernels for this device\n"
"  g_device_kernels[device_id] = kernels;\n"
"\n"
"  return CUDA_SUCCESS;\n"
"}}\n"
"\n"
"// TMA descriptor initialization (host-side)\n"
"{tma_init_func}\n"
"\n"
"// Main kernel launcher\n"
"extern \"C\" CUresult launch_kernel({launch_func_sig}, uint64_t _stream, int "
"device_id, tvm::ffi::Bytes cubin_path) {{\n"
"  CUresult result;\n"
"\n"
"  std::string cubin_path_str(reinterpret_cast<const char*>(cubin_path."
"data()), cubin_path.size());\n"
"  result = tilelang_init_cuda_module(cubin_path_str, device_id);\n"
"  if (result != CUDA_SUCCESS) return result;\n"
"\n"
"  result = tilelang_init_kernels(device_id);\n"
"  if (result != CUDA_SUCCESS) return result;\n"
"\n"
"{get_ptr_code}\n"
"  CUstream stream = (CUstream)_stream;\n"
"\n"
"{tma_init_in_launch}\n"
"\n"
"{kernel_launches}\n"
"\n"
"  return CUDA_SUCCESS;\n"
"}}\n"
"\n"
"// Cleanup function\n"
"extern \"C\" CUresult cleanup_module() {{\n"
"  std::lock_guard<std::mutex> lock(g_devices_mutex);\n"
"\n"
"  CUresult last_error = CUDA_SUCCESS;\n"
"\n"
"  // Step 1: Unload modules for all devices\n"
"  for (auto& pair : g_device_modules) {{\n"
"    if (pair.second != nullptr) {{\n"
"      CUresult result = cuModuleUnload(pair.second);\n"
"      if (result != CUDA_SUCCESS) {{\n"
"        std::cerr << \"Failed to unload module for device \" << pair.first\n"
"                  << \": \" << result << \"\\n\";\n"
"        last_error = result;\n"
"        // Continue cleanup even if unload fails\n"
"      }}\n"
"    }}\n"
"  }}\n"
"\n"
"  // Step 2: Release primary contexts (must execute even if module unload "
"failed)\n"
"  // This ensures the reference count is decremented for every "
"cuDevicePrimaryCtxRetain\n"
"  for (auto& pair : g_device_contexts) {{\n"
"    int device_id = pair.first;\n"
"    CUcontext ctx = pair.second;\n"
"\n"
"    if (ctx != nullptr) {{\n"
"      CUdevice device;\n"
"      CUresult result = cuDeviceGet(&device, device_id);\n"
"      if (result == CUDA_SUCCESS) {{\n"
"        result = cuDevicePrimaryCtxRelease(device);\n"
"        if (result != CUDA_SUCCESS) {{\n"
"          std::cerr << \"Failed to release primary context for device \"\n"
"                    << device_id << \": \" << result << \"\\n\";\n"
"          last_error = result;\n"
"        }}\n"
"      }} else {{\n"
"        std::cerr << \"Failed to get device \" << device_id\n"
"                  << \" for context release: \" << result << \"\\n\";\n"
"        last_error = result;\n"
"      }}\n"
"    }}\n"
"  }}\n"
"\n"
"  // Step 3: Clear all maps\n"
"  g_device_modules.clear();\n"
"  g_device_kernels.clear();\n"
"  g_device_contexts.clear();\n"
"\n"
"  return last_error;\n"
"}}\n"
"\n"
"TVM_FFI_DLL_EXPORT_TYPED_FUNC(launch_kernel, launch_kernel);\n"
"TVM_FFI_DLL_EXPORT_TYPED_FUNC(cleanup_module, cleanup_module);\n"
"\"\"\""
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:598
msgid ""
"\"\"\"    {function_name}({call_args}).launch(\n"
"      grid=[{grid_x}, {grid_y}, {grid_z}],\n"
"      block=[{block_x}, {block_y}, {block_z}],\n"
"      smem={smem_size},\n"
"      stream=stream,\n"
"    )\"\"\""
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:624
msgid ""
"\"\"\"{lib_code}\n"
"\n"
"  @cute.jit\n"
"  def kernel_wrapper({wrapper_args}):\n"
"{tma_init_code}{kernel_launches}\n"
"\n"
"  # Compile kernels to generate cubin\n"
"{fake_tensor_code}{fake_tma_tensor_code}  __fake_stream__ = "
"make_fake_stream()\n"
"  # Always generate cubin under a unique staging directory to avoid "
"concurrent\n"
"  # processes clobbering each other's intermediate artifacts.\n"
"  _staging_dir = Path(tempfile.mkdtemp(\n"
"      prefix=Path(__file__).stem + \".cubin.staging.\",\n"
"      dir=_module_dir,\n"
"  ))\n"
"  try:\n"
"    _kernel_wrapper = cute.compile(\n"
"        kernel_wrapper,\n"
"        {compile_args},\n"
"        options=f\"--enable-tvm-ffi --keep-cubin --dump-dir={{_staging_dir."
"as_posix()}}\",\n"
"    )\n"
"\n"
"    # CuTeDSL generates a long, mangled cubin filename that includes "
"argument/type info,\n"
"    # e.g. \"cutlass_kernel_wrapper_FakeTensor...sm_90a.cubin\". We expect "
"exactly one cubin.\n"
"    _cubin_files = sorted(_staging_dir.glob(\"*.cubin\"), key=lambda p: p."
"stat().st_mtime)\n"
"    if len(_cubin_files) != 1:\n"
"      raise RuntimeError(\n"
"          f\"Expected exactly one .cubin under {{_staging_dir}}, got "
"{{len(_cubin_files)}}: {{_cubin_files}}\"\n"
"      )\n"
"    os.replace(_cubin_files[0], _cubin_path)\n"
"  finally:\n"
"    shutil.rmtree(_staging_dir, ignore_errors=True)\"\"\""
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:671
msgid ""
"\"\"\"import os\n"
"from pathlib import Path\n"
"\n"
"# Minimal imports for runtime (no cutlass/cute - only needed for cubin "
"generation)\n"
"import tvm.runtime as runtime\n"
"\n"
"_cpp_launcher = None\n"
"_cpp_launcher_lib = None\n"
"_cubin_generated = False\n"
"\n"
"# Pre-compute paths - cubin is stored alongside the launcher .so\n"
"# Use module basename to avoid conflicts when multiple kernels run "
"concurrently\n"
"# e.g., \"/tmp/tmp8liu__ho.py\" -> \"/tmp/tmp8liu__ho.cubin\"\n"
"#       \"kernel.py\" (in cache) -> \"kernel.cubin\"\n"
"_module_dir = Path(os.path.dirname(__file__))\n"
"_cubin_path = _module_dir / (Path(__file__).stem + \".cubin\")\n"
"_cubin_path_bytes = _cubin_path.as_posix().encode('utf-8')\n"
"_cubin_needs_generation = not _cubin_path.exists()\n"
"\n"
"def _generate_cubin_if_needed({cubin_gen_params}):\n"
"  \"\"\"Generate cubin file on first call.\n"
"\n"
"  All CuTeDSL imports are inside this function to avoid slow\n"
"  module-level initialization when loading from cache.\n"
"  \"\"\"\n"
"  global _cubin_generated, _cubin_path\n"
"\n"
"  # Lazy import CuTeDSL only when cubin generation is needed\n"
"  from cuda.bindings.driver import CUstream\n"
"  import cutlass\n"
"  import cutlass.cute as cute\n"
"  from cutlass.cute.runtime import make_fake_stream, "
"make_fake_compact_tensor\n"
"  import tilelang.contrib.cutedsl as tl\n"
"  # We rely on CuTeDSL's keep-cubin artifact rather than custom extraction.\n"
"  import tempfile\n"
"  import shutil\n"
"\n"
"  _DTYPE_MAP = {{\n"
"      \"torch.float32\": cutlass.Float32,\n"
"      \"torch.float16\": cutlass.Float16,\n"
"      \"torch.bfloat16\": cutlass.BFloat16,\n"
"      \"torch.float8_e4m3fnuz\": cutlass.Float8E4M3FN,\n"
"      \"torch.float8_e4m3fn\": cutlass.Float8E4M3FN,\n"
"      \"torch.float8_e5m2\": cutlass.Float8E5M2,\n"
"      \"torch.float64\": cutlass.Float64,\n"
"      \"torch.int64\": cutlass.Int64,\n"
"      \"torch.int32\": cutlass.Int32,\n"
"      \"torch.uint32\": cutlass.Uint32,\n"
"      \"torch.bool\": cutlass.Boolean,\n"
"      \"torch.int8\": cutlass.Int8,\n"
"      \"torch.uint8\": cutlass.Uint8,\n"
"      \"torch.int16\": cutlass.Int16,\n"
"      \"torch.uint16\": cutlass.Uint16,\n"
"      \"torch.uchar\": cutlass.Uint8,\n"
"  }}\n"
"\n"
"{cubin_gen_code}\n"
"\n"
"  _cubin_generated = True\n"
"\n"
"def _load_cpp_launcher():\n"
"  \"\"\"Load C++ kernel launcher.\"\"\"\n"
"  global _cpp_launcher, _cpp_launcher_lib\n"
"  if _cpp_launcher is not None:\n"
"    return _cpp_launcher\n"
"\n"
"  lib_path = os.path.join(os.path.dirname(__file__), "
"\"{launcher_lib_name}\")\n"
"  if not os.path.exists(lib_path):\n"
"    raise FileNotFoundError(f\"Launcher not found: {{lib_path}}\")\n"
"\n"
"  _cpp_launcher_lib = runtime.load_module(lib_path)\n"
"  _cpp_launcher = _cpp_launcher_lib[\"launch_kernel\"]\n"
"  return _cpp_launcher\n"
"\n"
"def call({call_func_params}, stream, device_id=0):\n"
"  \"\"\"Kernel dispatch function.\n"
"\n"
"  Args:\n"
"      stream: CUDA stream handle\n"
"      device_id: CUDA device ID (should be passed from caller, defaults to 0 "
"for backward compatibility)\n"
"  \"\"\"\n"
"  global _cubin_path_bytes, _cubin_needs_generation\n"
"\n"
"  if _cubin_needs_generation:\n"
"    _generate_cubin_if_needed({cubin_gen_call_args})\n"
"    _cubin_needs_generation = False\n"
"\n"
"{arg_prep_code}\n"
"\n"
"  launcher = _load_cpp_launcher()\n"
"  result = launcher({launcher_call_args}, stream, device_id, "
"_cubin_path_bytes)\n"
"\n"
"  if result != 0:\n"
"    raise RuntimeError(f\"Kernel launch failed with CUDA error "
"{{result}}\")\n"
"\"\"\""
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:777
msgid "Bases: :py:obj:`tilelang.jit.adapter.wrapper.TLCUDASourceWrapper`"
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:782
msgid ""
"Generates optimized C++ launcher code that: - Loads cubin via CUDA Driver "
"API - Passes TMA descriptors by value (host-side, no device copy) - Launches "
"kernels with minimal Python overhead - Supports both single and multiple "
"kernel scenarios"
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:0
msgid "Parameters"
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:791
msgid "Override parent's host_func to return generated Python code."
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:796
msgid "Generate TMA descriptor information for C++ code generation."
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:0
msgid "Returns"
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:798
msgid "List of descriptor variable names in the order they were processed."
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:0
msgid "Return type"
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:804
msgid "Create dispatch function - always use C++ launcher."
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:810
msgid "Create dispatch function using C++ launcher."
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:816
msgid "Get the generated C++ launcher code."
msgstr ""

#: ../../../autoapi/tilelang/jit/adapter/cutedsl/wrapper/index.rst:822
msgid "Update the library code with the given code string."
msgstr ""
