# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025-2025, Tile Lang Contributors
# This file is distributed under the same license as the Tile Language <br> package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Tile Language <br> 0.1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-14 17:52+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../deeplearning_operators/deepseek_mla.md:1
msgid "ðŸš€ Write High Performance FlashMLA with TileLang on Hopper"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:4
msgid ""
"<div style=\"text-align: left;\">\n"
"    <em>Author:</em> <a href=\"https://github.com/chengyupku\">Yu Cheng</"
"a> \n"
"    <em>Author:</em> <a href=\"https://github.com/LeiWang1999\">Lei Wang</"
"a>\n"
"</div>\n"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:9
msgid ""
"TileLang is a user-friendly AI programming language that significantly "
"lowers the barrier to kernel programming, helping users quickly build "
"customized operators. However, users still need to master certain "
"programming techniques to better leverage TileLang's powerful capabilities. "
"Here, we'll use MLA as an example to demonstrate how to write high-"
"performance kernels with TileLang."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:11
msgid "Introduction to MLA"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:13
msgid ""
"DeepSeek's MLA (Multi-Head Latent Attention) is a novel attention mechanism "
"known for its hardware efficiency and significant improvements in model "
"inference speed. Several deep learning compilers (such as [Triton](https://"
"github.com/triton-lang/triton)) and libraries (such as [FlashInfer](https://"
"github.com/flashinfer-ai/flashinfer)) have developed their own "
"implementations of MLA. In February 2025, [FlashMLA](https://github.com/"
"deepseek-ai/FlashMLA) was open-sourced on GitHub. FlashMLA utilizes [CUTLASS]"
"(https://github.com/NVIDIA/cutlass) templates and incorporates optimization "
"techniques from [FlashAttention](https://github.com/Dao-AILab/flash-"
"attention), achieving impressive performance."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:15
msgid "Benchmark Results"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:17
msgid ""
"We benchmarked the performance of FlashMLA, TileLang, Torch, Triton, and "
"FlashInfer under batch sizes of 64 and 128, with float16 data type, as shown "
"in the figures below."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:19
#: ../../../deeplearning_operators/deepseek_mla.md:27
#: ../../../deeplearning_operators/deepseek_mla.md:75
#: ../../../deeplearning_operators/deepseek_mla.md:83
msgid "Overview"
msgstr "æ¦‚è¦½"

#: ../../../deeplearning_operators/deepseek_mla.md:24
msgid "Figure 1: Performance under batch size=64"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:32
msgid "Figure 2: Performance under batch size=128"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:35
msgid ""
"As shown in the results, TileLang achieves performance comparable to "
"FlashMLA in most cases, significantly outperforming both FlashInfer and "
"Triton.  Notably, **TileLang accomplishes this with just around 80 lines of "
"Python code**, demonstrating its exceptional ease of use and efficiency. "
"Let's dive in and see how TileLang achieves this."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:38
msgid "Implementation"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:40
msgid ""
"First, let's review the core computation logic of traditional FlashAttention:"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:42
msgid ""
"# acc_s: [block_M, block_N]\n"
"# scores_max: [block_M]\n"
"# scores_scale: [block_M]\n"
"# acc_o: [block_M, dim]\n"
"\n"
"for i in range(loop_range):\n"
"    acc_s = Q @ K[i]\n"
"    scores_max_prev = scores_max\n"
"    scores_max = max(acc_s, dim=1)\n"
"    scores_scale = exp(scores_max_prev - scores_max)\n"
"    acc_o *= scores_scale\n"
"    acc_s = exp(acc_s - scores_max)\n"
"    acc_o = acc_s @ V[i]\n"
"    ...\n"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:59
msgid ""
"Here, `acc_s` represents the `Q @ K` result in each iteration with "
"dimensions `[block_M, block_N]`, while `acc_o` represents the current "
"iteration's output with dimensions `[block_M, dim]`. Both `acc_s` and "
"`acc_o` need to be stored in registers to reduce latency."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:61
msgid ""
"Compared to traditional attention operators like MHA (Multi-Headed "
"Attention) or GQA (Grouped Query Attention), a major challenge in optimizing "
"MLA is its large head dimensions - `query` and `key` have head dimensions of "
"576 (512 + 64), while `value` has a head dimension of 512. This raises a "
"significant issue: `acc_o` becomes too large, and with insufficient threads "
"(e.g., 128 threads), register spilling occurs, severely impacting "
"performance."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:63
msgid ""
"This raises the question of how to partition the matrix multiplication "
"operation. On the Hopper architecture, most computation kernels use [`wgmma."
"mma_async`](https://docs.nvidia.com/cuda/parallel-thread-execution/"
"#asynchronous-warpgroup-level-matrix-instructions) instructions for optimal "
"performance. The `wgmma.mma_async` instruction organizes 4 warps (128 "
"threads) into a warpgroup for collective MMA operations. However, `wgmma."
"mma_async` instructions require a minimum M dimension of 64. This means each "
"warpgroup's minimum M dimension can only be reduced to 64, but a tile size "
"of 64*512 is too large for a single warpgroup, leading to register spilling."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:65
msgid ""
"Therefore, our only option is to partition `acc_o` along the `dim` "
"dimension, with two warpgroups computing the left and right part of `acc_o` "
"respectively. However, this introduces another challenge: both warpgroups "
"require the complete `acc_s` result as input."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:67
msgid ""
"Our solution is to have each warpgroup compute half of `acc_s` during `Q @ "
"K` computation, then obtain the other half computed by the other warpgroup "
"through shared memory."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:69
msgid "Layout Inference"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:71
msgid ""
"While the above process may seem complex, but don't worry - TileLang will "
"handle all these intricacies for you."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:73
msgid ""
"Figure 3 and Figure 4 illustrate the frontend TileLang script and its "
"corresponding execution plan for MLA. Here, `T.gemm` represents matrix "
"multiplication operations, `transpose_B=True` indicates transposition of "
"matrix B, and `policy=FullCol` specifies that each warpgroup computes one "
"column (e.g., split the result matrix in vertical dimension). `T.copy` "
"represents buffer-to-buffer copying operations."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:80
msgid "Figure 3: Buffer shapes in Q @ K"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:88
msgid "Figure 4: Buffer shapes in acc_s @ V"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:91
msgid ""
"The mapping from TileLang frontend code to execution plan is accomplished "
"through Layout Inference. Layout inference is a core optimization technique "
"in TileLang. It automatically deduces the required buffer shapes and optimal "
"layouts based on Tile-Operators (like `T.gemm`, `T.copy`, etc.), then "
"generates the corresponding code. Here, we demonstrate a concrete example of "
"buffer shape inference in MLA."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:93
msgid ""
"For instance, when computing `Q @ K`, TileLang infers that each warpgroup's "
"`acc_s_0` shape should be `[blockM, blockN / 2]` based on the "
"`policy=FullCol` annotation in `T.gemm`. Since this is followed by an `acc_s "
"@ V` operation with `policy=FullCol`, which requires each warpgroup to have "
"the complete `acc_s` result, TileLang deduces that `acc_s`'s shape at this "
"point should be `[blockM, blockN]`. Consequently, TileLang can continue the "
"inference process forward, determining that both `S_shared` and `acc_s` in "
"`T.copy(S_shared, acc_s)` should have shapes of `[blockM, blockN]`."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:95
msgid ""
"It's worth noting that our scheduling approach differs from FlashMLA's "
"implementation strategy. In FlashMLA, `Q @ K` is assigned to a single "
"warpgroup, while the `acc_o` partitioning scheme remains consistent with "
"ours. Nevertheless, our scheduling approach still achieves comparable "
"performance."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:97
msgid "Threadblock Swizzling"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:99
msgid ""
"Threadblock swizzling is a common performance optimization technique in GPU "
"kernel optimization. In GPU architecture, the L2 cache is a high-speed cache "
"shared among multiple SMs (Streaming Multiprocessors). Threadblock swizzling "
"optimizes data access patterns by remapping the scheduling order of "
"threadblocks, thereby improving L2 cache hit rates. Traditional scheduling "
"typically executes threadblocks in the natural order of the grid, which can "
"lead to non-contiguous data access patterns between adjacent threadblocks, "
"resulting in inefficient utilization of cached data. The swizzle technique "
"employs mathematical mapping methods (such as diagonal or interleaved "
"mapping) to adjust the execution order of threadblocks, ensuring that "
"consecutively scheduled threadblocks access adjacent or overlapping data "
"regions."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:101
msgid ""
"In TileLang, threadblock swizzling optimization can be implemented with just "
"a single line of Python code:"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:103
msgid "T.use_swizzle(panel_size: int, order: str = \"row\")\n"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:107
msgid ""
"Here, `panel_size` specifies the width of the swizzled threadblock group, "
"and `order` determines the swizzling pattern, which can be either \"row\" or "
"\"col\"."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:110
msgid "Shared Memory Swizzling"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:112
msgid ""
"In CUDA programming, shared memory is divided into multiple memory banks, "
"with each bank capable of servicing one thread request per clock cycle in "
"parallel. Bank conflicts occur when multiple threads simultaneously access "
"different addresses mapped to the same bank, forcing these accesses to be "
"serialized and degrading performance."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:114
msgid ""
"One common strategy to address bank conflicts is shared memory swizzling. "
"This technique rearranges how data is stored in shared memory by remapping "
"addresses that would originally fall into the same bank to different banks, "
"thereby reducing conflicts. For example, XOR operations or other bit "
"manipulations can be incorporated into address calculations to alter the "
"data layout, resulting in more evenly distributed memory accesses across "
"consecutive threads. This approach is particularly crucial for implementing "
"high-performance computing tasks like matrix multiplication and convolution, "
"as it can significantly improve memory access parallelism and overall "
"execution efficiency."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:116
msgid ""
"Similarly, TileLang also supports shared memory swizzling. Users only need "
"to add a single line of Python code:"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:118
msgid ""
"T.annotate_layout({\n"
"    S_shared: TileLang.layout.make_swizzled_layout(S_shared),\n"
"})\n"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:124
msgid ""
"Here, `T.annotate_layout` allows users to specify any desired layout for a "
"buffer. For convenience, TileLang provides the `make_swizzled_layout` "
"primitive to automatically generate a swizzled layout."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:127
msgid "Warp-Specialization"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:129
msgid ""
"The Hopper architecture commonly employs warp specialization for performance "
"optimization. A typical approach is to designate one warpgroup as a producer "
"that handles data movement using TMA (Tensor Memory Accelerator), while the "
"remaining warpgroups serve as consumers performing computations. However, "
"this programming pattern is complex, requiring developers to manually manage "
"the execution logic for producers and consumers, including synchronization "
"through the `mbarrier` objects."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:131
msgid ""
"In TileLang, users are completely shielded from these implementation "
"details. The frontend script is automatically transformed into a warp-"
"specialized form, where TileLang handles all producer-consumer "
"synchronization automatically, enabling efficient computation."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:134
msgid "Pipeline"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:137
msgid ""
"Pipeline is a technique used to improve memory access efficiency by "
"overlapping memory access and computation. In TileLang, pipeline can be "
"implemented through the `T.pipelined` annotation:"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:139
msgid "T.pipelined(range: int, stage: int)\n"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:143
msgid ""
"Here, `range` specifies the range of the pipeline, and `stage` specifies the "
"stage of the pipeline. Multi-stage pipelining enables overlapping of "
"computation and memory access, which can significantly improve performance "
"for memory-intensive operators. However, setting a higher number of stages "
"consumes more shared memory resources, so the optimal configuration needs to "
"be determined based on specific use cases."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:146
msgid "Split-KV"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:148
msgid ""
"We have also implemented Split-KV optimization similar to [FlashDecoding]"
"(https://pytorch.org/blog/flash-decoding/). Specifically, when the batch "
"size is small, parallel SM resources cannot be fully utilized due to low "
"parallelism. In such cases, we can split the kv_ctx dimension across "
"multiple SMs for parallel computation and then merge the results."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:150
msgid ""
"In our implementation, we have developed both split and combine kernels, "
"allowing users to control the split size through a `num_split` parameter."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:153
msgid "ðŸš€ On AMD MI300X Accelerators"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:155
msgid ""
"Following our previous demonstration of [high-performance FlashMLA "
"implementation on NVIDIA Hopper architectures using TileLang](https://github."
"com/tile-ai/tilelang/blob/main/examples/deepseek_mla/README.md), this work "
"presents an optimized implementation for AMD MI300X accelerators. We examine "
"architectural differences and corresponding optimization strategies between "
"these platforms."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:157
msgid "Architectural Considerations and Optimization Strategies"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:159
msgid ""
"Key implementation differences between Hopper and MI300X architectures "
"include:"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:161
msgid ""
"**Instruction Set Variations**: The MI300X architecture eliminates the need "
"for explicit Tensor Memory Access (TMA) instructions and warp "
"specialization, which are automatically handled by the compiler on Hopper "
"architectures, resulting in identical source code manifestations."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:163
msgid ""
"**Shared Memory Constraints**: With 64KB of shared memory compared to "
"Hopper's 228KB, MI300X implementations require careful memory management. "
"Our optimization strategy includes:"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:164
msgid "Reducing software pipeline stages"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:165
msgid ""
"Register-based caching of Q matrices instead of shared memory utilization:"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:166
msgid ""
"# Original shared memory allocation\n"
"Q_shared = T.alloc_shared([block_H, dim], dtype)\n"
"Q_pe_shared = T.alloc_shared([block_H, pe_dim], dtype)\n"
"\n"
"# Optimized register allocation\n"
"Q_local = T.alloc_fragment([block_H, dim], dtype)\n"
"Q_pe_local = T.alloc_fragment([block_H, pe_dim], dtype)\n"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:176
msgid ""
"**Tile Size Flexibility**: The absence of WGMMA instructions on MI300X "
"permits more flexible tile size selection, removing the requirement for "
"block_m to be multiples of 64."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:178
msgid ""
"**Memory Bank Conflict Swizzling**: MI300x has different memory bank "
"conflict rules compared to NVIDIA, so we need to use different swizzling "
"strategies. This is also automatically handled by TileLang, resulting in no "
"visible differences in the code."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:180
msgid "Performance Evaluation"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:182
msgid ""
"We conducted comparative performance analysis across multiple frameworks "
"using float16 precision with batch sizes 64 and 128. The experimental "
"results demonstrate:"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:184
msgid ""
"<figure style=\"text-align: center\">\n"
"  <a href=\"../figures/flashmla-amd.png\">\n"
"    <img src=\"../figures/flashmla-amd.png\" alt=\"AMD FlashMLA Performance "
"Comparison\">\n"
"   </a>\n"
"  <figcaption style=\"text-align: center;\">Figure 1: Computational "
"throughput comparison across frameworks (Batch sizes 64 and 128)</"
"figcaption>\n"
"</figure>\n"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:191
msgid ""
"Notably, TileLang achieves performance parity with hand-optimized assembly "
"kernels (aiter-asm) in most test cases, while significantly outperforming "
"both Triton (1.98Ã—) and PyTorch (3.76Ã—) implementations. This performance is "
"achieved through a concise 80-line Python implementation, demonstrating "
"TileLang's efficiency and programmability advantages."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:193
msgid "Future Optimization Opportunities"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:195
msgid ""
"**Memory Bank Conflict Mitigation**: Current implementations primarily "
"address bank conflicts in NT layouts through TileLang's automatic "
"optimization. Further investigation of swizzling techniques for alternative "
"memory layouts remains an open research direction."
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:197
msgid ""
"**Dimension Parallelization**: For large MLA dimensions (e.g., 576 "
"elements), we propose investigating head dimension partitioning strategies "
"to:"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:198
msgid "Reduce shared memory pressure"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:199
msgid "Improve compute-to-memory access ratios"
msgstr ""

#: ../../../deeplearning_operators/deepseek_mla.md:200
msgid "Enhance parallelism through dimension-wise task distribution"
msgstr ""
