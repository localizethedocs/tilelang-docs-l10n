# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025-2025, Tile Lang Contributors
# This file is distributed under the same license as the Tile Language <br> package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TileLang <br> 0.1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-02-17 08:38+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../deeplearning_operators/gemv.md:1
msgid "General Matrix-Vector Multiplication (GEMV)"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:2
msgid "==========================================="
msgstr ""

#: ../../../deeplearning_operators/gemv.md:4
msgid ""
"<div style=\"text-align: left;\">\n"
"    <em>Contributor: </em> <a href=\"https://github.com/botbw\">@botbw</a>\n"
"</div>\n"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:9
msgid ""
"This document is still **experimental** and may be incomplete.    "
"Suggestions and improvements are highly encouraged—please submit a PR!"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:14
msgid "Example code can be found at `examples/gemv/example_gemv.py`."
msgstr ""

#: ../../../deeplearning_operators/gemv.md:17
msgid ""
"General matrix-vector multiplication (GEMV) can be viewed as a specialized "
"case of general matrix-matrix multiplication (GEMM). It plays a critical "
"role in deep learning, especially during the inference phase of large "
"language models. In this tutorial, we will optimize GEMV from a thread-level "
"perspective step by step using `TileLang`."
msgstr ""

#: ../../../deeplearning_operators/gemv.md:19
msgid "Triton Implementation"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:20
msgid ""
"When implementing a GEMV kernel, you might start with a high-level approach "
"using a tool like `Triton`."
msgstr ""

#: ../../../deeplearning_operators/gemv.md:22
msgid "A simple Triton kernel for GEMV might look like this:"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:23
msgid ""
"@triton.jit\n"
"def _gemv_naive(\n"
"    x_ptr, A_ptr, y_ptr,\n"
"    N, K,\n"
"    BLOCK_SIZE_K: tl.constexpr,\n"
"):\n"
"    n = tl.program_id(0)\n"
"    offs_k = tl.arange(0, BLOCK_SIZE_K)\n"
"    mask = offs_k < K\n"
"    a_ptrs = A_ptr + n * K + offs_k\n"
"    a_vals = tl.load(a_ptrs, mask=mask, other=0.0)\n"
"    x_vals = tl.load(x_ptr + offs_k, mask=mask, other=0.0)\n"
"    dot = tl.sum(a_vals * x_vals, axis=0)\n"
"    tl.store(y_ptr + n, dot)\n"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:40
msgid ""
"`Triton` is straightforward to use, as it operates at the block level. "
"However, this approach may not allow for fine-grained thread-level "
"optimization. In this tutorial, we will demonstrate how to write an "
"optimized GEMV kernel in `TileLang` that exposes more low-level control."
msgstr ""

#: ../../../deeplearning_operators/gemv.md:42
msgid "Naive Implementation in TileLang"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:43
msgid ""
"If you have a basic understanding of CUDA C, it is natural to start with a "
"naive GEMV kernel by adapting a GEMM tiling strategy. You can think of GEMV "
"as a `(1, k) * (k, n)` GEMM. Below is a simple example:"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:45
msgid ""
"def naive_gemv(\n"
"    N: int,\n"
"    K: int,\n"
"    BLOCK_N: int,\n"
"    BLOCK_K: int,\n"
"    dtype: str = \"float16\",\n"
"    accum_dtype: str = \"float\",\n"
"):\n"
"\n"
"    @T.prim_func\n"
"    def main(\n"
"            A: T.Buffer((K,), dtype),\n"
"            B: T.Buffer((N, K), dtype),\n"
"            C: T.Buffer((N,), dtype),\n"
"    ):\n"
"        with T.Kernel(T.ceildiv(N, BLOCK_N)) as bn:\n"
"            tn = T.get_thread_binding(0)  # tn = threadIdx.x\n"
"            A_shared = T.alloc_shared((BLOCK_K,), dtype)\n"
"            B_shared = T.alloc_shared((BLOCK_N, BLOCK_K), dtype)\n"
"            C_reg = T.alloc_local((1,), accum_dtype)\n"
"            T.clear(C_reg)\n"
"            for bk in T.serial(T.ceildiv(K, BLOCK_K)):\n"
"                for tk in T.serial(BLOCK_K):\n"
"                    A_shared[tk] = A[bk * BLOCK_K + tk]\n"
"                    B_shared[tn, tk] = B[bn * BLOCK_N + tn, bk * BLOCK_K + "
"tk]\n"
"                for tk in T.serial(BLOCK_K):\n"
"                    C_reg[0] += A_shared[tk].astype(accum_dtype) * "
"B_shared[tn,\n"
"                                                                            tk]."
"astype(accum_dtype)\n"
"            C[bn * BLOCK_N + tn] = C_reg[0]\n"
"\n"
"    return main\n"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:79
msgid ""
"And your kernel will be compiled into CUDA by `TileLang` (in `~/.tilelang/"
"cache`):"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:81
msgid ""
"extern \"C\" __global__ void __launch_bounds__(256, 1) main_kernel(half_t* "
"__restrict__ A, half_t* __restrict__ B, half_t* __restrict__ C) {\n"
"  extern __shared__ __align__(1024) uchar buf_dyn_shmem[];\n"
"  float C_reg[1];\n"
"  __shared__ uint64_t _mbarrier[2];\n"
"  if (((int)threadIdx.x) == 0) {\n"
"    tl::mbarrier_init(_mbarrier[0], 128);\n"
"    tl::mbarrier_init(_mbarrier[1], 128);\n"
"  }\n"
"  __syncthreads();\n"
"  if (128 <= ((int)threadIdx.x)) {\n"
"    tl::warpgroup_reg_dealloc<24>();\n"
"    for (int bk = 0; bk < 8; ++bk) {\n"
"      tl::mbarrier_wait(_mbarrier[1], ((bk & 1) ^ 1));\n"
"      for (int tk = 0; tk < 128; ++tk) {\n"
"        ((half_t*)buf_dyn_shmem)[tk] = A[((bk * 128) + tk)];\n"
"        ((half_t*)buf_dyn_shmem)[(((((int)threadIdx.x) * 128) + tk) - "
"16256)] = B[(((((((int)blockIdx.x) * 131072) + (((int)threadIdx.x) * 1024)) "
"+ (bk * 128)) + tk) - 131072)];\n"
"      }\n"
"      tl::fence_proxy_async();\n"
"      tl::mbarrier_cp_async_arrive(_mbarrier[0]);\n"
"      tl::mbarrier_arrive(_mbarrier[0]);\n"
"    }\n"
"  } else {\n"
"    tl::warpgroup_reg_alloc<240>();\n"
"    C_reg[0] = 0.000000e+00f;\n"
"    for (int bk_1 = 0; bk_1 < 8; ++bk_1) {\n"
"      tl::mbarrier_wait(_mbarrier[0], (bk_1 & 1));\n"
"      for (int tk_1 = 0; tk_1 < 128; ++tk_1) {\n"
"        C_reg[0] = (C_reg[0] + (((float)((half_t*)buf_dyn_shmem)[tk_1]) * "
"((float)((half_t*)buf_dyn_shmem)[(((((int)threadIdx.x) * 128) + tk_1) + "
"128)])));\n"
"      }\n"
"      tl::fence_proxy_async();\n"
"      tl::mbarrier_arrive(_mbarrier[1]);\n"
"    }\n"
"    C[((((int)blockIdx.x) * 128) + ((int)threadIdx.x))] = "
"((half_t)C_reg[0]);\n"
"  }\n"
"}\n"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:119
msgid ""
"In this design, the first 128 threads act as the data producer and the last "
"128 threads as the consumer within a block (assuming a 1D block)."
msgstr ""

#: ../../../deeplearning_operators/gemv.md:121
msgid ""
"At this level, we only gain very little computation power from our GPU with "
"around **~0.17 ms** compared to torch/cuBLAS's **~0.008 ms**, which is "
"around 20x slower."
msgstr ""

#: ../../../deeplearning_operators/gemv.md:123
msgid "More Concurrency"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:125
msgid ""
"To further increase the concurrency of our kernel, we can exploit finer "
"thread-level parallelism. Instead of assigning each thread to compute a "
"single output element in C, you can introduce parallelism along the K "
"dimension. Each thread computes a partial accumulation, and you then combine "
"these partial results. This approach requires primitives like `atomicAdd` in "
"CUDA."
msgstr ""

#: ../../../deeplearning_operators/gemv.md:127
msgid "Here’s a simplified version:"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:128
msgid ""
"def naive_splitk_gemv(\n"
"    N: int,\n"
"    K: int,\n"
"    BLOCK_N: int,\n"
"    BLOCK_K: int,\n"
"    dtype: str = \"float16\",\n"
"    accum_dtype: str = \"float\",\n"
"):\n"
"\n"
"    @T.prim_func\n"
"    def main(\n"
"            A: T.Buffer((K,), dtype),\n"
"            B: T.Buffer((N, K), dtype),\n"
"            C: T.Buffer((N,), dtype),\n"
"    ):\n"
"        with T.Kernel(T.ceildiv(N, BLOCK_N), threads=(BLOCK_N, BLOCK_K)) as "
"bn:\n"
"            tn = T.get_thread_binding(0)\n"
"            tk = T.get_thread_binding(1)\n"
"            A_local = T.alloc_local((1,), dtype)\n"
"            B_local = T.alloc_local((1,), dtype)\n"
"            C_accum = T.alloc_local((1,), accum_dtype)\n"
"            C_shared = T.alloc_shared((BLOCK_N,), accum_dtype)\n"
"            if tk == 0:\n"
"                C_shared[tn] = 0\n"
"            T.clear(C_accum)\n"
"            for bk in T.serial(T.ceildiv(K, BLOCK_K)):\n"
"                A_local[0] = A[bk * BLOCK_K + tk]\n"
"                B_local[0] = B[bn * BLOCK_N + tn, bk * BLOCK_K + tk]\n"
"                C_accum[0] += A_local[0].astype(accum_dtype) * B_local[0]."
"astype(accum_dtype)\n"
"            T.atomic_add(C_shared[tn], C_accum[0])\n"
"            C[bn * BLOCK_N + tn] = C_shared[tn]\n"
"\n"
"    return main\n"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:164
msgid ""
"By introducing parallelism along K dimension, our kernel now achieves "
"**~0.024 ms**, an improvement, but still not on par with torch/cuBLAS."
msgstr ""

#: ../../../deeplearning_operators/gemv.md:166
msgid "Customizing Parallelism in K Dimension"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:167
msgid ""
"If your K dimension is large, you can further customize how many elements "
"each thread processes by introducing a `reduce_threads` parameter. This way, "
"each thread handles multiple elements per iteration:"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:169
msgid ""
"def splitk_gemv(\n"
"    N: int,\n"
"    K: int,\n"
"    BLOCK_N: int,\n"
"    BLOCK_K: int,\n"
"    reduce_threads: int,\n"
"    dtype: str = \"float16\",\n"
"    accum_dtype: str = \"float\",\n"
"):\n"
"    TILE_K = T.ceildiv(BLOCK_K, reduce_threads)\n"
"\n"
"    @T.prim_func\n"
"    def main(\n"
"            A: T.Buffer((K,), dtype),\n"
"            B: T.Buffer((N, K), dtype),\n"
"            C: T.Buffer((N,), dtype),\n"
"    ):\n"
"        with T.Kernel(T.ceildiv(N, BLOCK_N), threads=(BLOCK_N, "
"reduce_threads)) as bn:\n"
"            tn = T.get_thread_binding(0)\n"
"            tk = T.get_thread_binding(1)\n"
"            A_local = T.alloc_local((TILE_K,), dtype)\n"
"            B_local = T.alloc_local((TILE_K,), dtype)\n"
"            C_shared = T.alloc_shared((BLOCK_N,), accum_dtype)\n"
"            C_accum = T.alloc_local((1,), accum_dtype)\n"
"            if tk == 0:\n"
"                C_shared[tn] = 0\n"
"            T.clear(C_accum)\n"
"            for bk in T.serial(T.ceildiv(K, BLOCK_K)):\n"
"                for k in T.serial(TILE_K):\n"
"                    A_local[k] = A[bk * BLOCK_K + tk * TILE_K + k]\n"
"                    B_local[k] = B[bn * BLOCK_N + tn, bk * BLOCK_K + tk * "
"TILE_K + k]\n"
"                for k in T.serial(TILE_K):\n"
"                    C_accum[0] += A_local[k].astype(accum_dtype) * "
"B_local[k].astype(accum_dtype)\n"
"            T.atomic_add(C_shared[tn], C_accum[0])\n"
"            C[bn * BLOCK_N + tn] = C_shared[tn]\n"
"\n"
"    return main\n"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:209
msgid "Vectorized Reads"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:211
msgid ""
"GEMV is less computation intensive than GEMM as the computation intensity "
"and memory throughput will be the optimization bottleneck. One effective "
"strategy is to use vectorized load/store operations (e.g., `float2`, "
"`float4`). In `TileLang`, you can specify vectorized operations via `T."
"vectorized`:"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:213
msgid ""
"def splitk_gemv_vectorized(\n"
"    N: int,\n"
"    K: int,\n"
"    BLOCK_N: int,\n"
"    reduce_threads: int,\n"
"    dtype: str = \"float16\",\n"
"    accum_dtype: str = \"float\",\n"
"):\n"
"    MAX_TRANSACTION_SIZE_IN_BITS = 128\n"
"    TILE_K = MAX_TRANSACTION_SIZE_IN_BITS // DataType(dtype).bits\n"
"    BLOCK_K = reduce_threads * TILE_K\n"
"\n"
"    @T.prim_func\n"
"    def main(\n"
"            A: T.Buffer((K,), dtype),\n"
"            B: T.Buffer((N, K), dtype),\n"
"            C: T.Buffer((N,), dtype),\n"
"    ):\n"
"        with T.Kernel(T.ceildiv(N, BLOCK_N), threads=(BLOCK_N, "
"reduce_threads)) as bn:\n"
"            tn = T.get_thread_binding(0)\n"
"            tk = T.get_thread_binding(1)\n"
"            A_local = T.alloc_local((TILE_K,), dtype)\n"
"            B_local = T.alloc_local((TILE_K,), dtype)\n"
"            C_shared = T.alloc_shared((BLOCK_N,), accum_dtype)\n"
"            C_accum = T.alloc_local((1,), accum_dtype)\n"
"            if tk == 0:\n"
"                C_shared[tn] = 0\n"
"            T.clear(C_accum)\n"
"            for bk in T.serial(T.ceildiv(K, BLOCK_K)):\n"
"                for k in T.vectorized(TILE_K):\n"
"                    A_local[k] = A[bk * BLOCK_K + tk * TILE_K + k]\n"
"                    B_local[k] = B[bn * BLOCK_N + tn, bk * BLOCK_K + tk * "
"TILE_K + k]\n"
"                for k in T.serial(TILE_K):\n"
"                    C_accum[0] += A_local[k].astype(accum_dtype) * "
"B_local[k].astype(accum_dtype)\n"
"            T.atomic_add(C_shared[tn], C_accum[0])\n"
"            C[bn * BLOCK_N + tn] = C_shared[tn]\n"
"\n"
"    return main\n"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:254
msgid ""
"With vectorized read, now the kernel finishes in **~0.0084 ms**, which is "
"getting close to cuBLAS performance."
msgstr ""

#: ../../../deeplearning_operators/gemv.md:256
msgid "`tvm_thread_allreduce` Instead of `atomicAdd`"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:258
msgid ""
"[`tvm_thread_allreduce`](https://tvm.apache.org/docs/reference/api/python/"
"tir/tir.html#tvm.tir.tvm_thread_allreduce) has implemented optimization when "
"making an all-reduce across a number of threads, which should outperfrom out "
"plain smem + `atomidAdd`:"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:260
msgid ""
"def splitk_gemv_vectorized_tvm(\n"
"    N: int,\n"
"    K: int,\n"
"    BLOCK_N: int,\n"
"    reduce_threads: int,\n"
"    dtype: str = \"float16\",\n"
"    accum_dtype: str = \"float\",\n"
"):\n"
"    MAX_TRANSACTION_SIZE_IN_BITS = 128\n"
"    TILE_K = MAX_TRANSACTION_SIZE_IN_BITS // DataType(dtype).bits\n"
"    BLOCK_K = reduce_threads * TILE_K\n"
"\n"
"    @T.prim_func\n"
"    def main(\n"
"            A: T.Buffer((K,), dtype),\n"
"            B: T.Buffer((N, K), dtype),\n"
"            C: T.Buffer((N,), dtype),\n"
"    ):\n"
"        with T.Kernel(T.ceildiv(N, BLOCK_N), threads=(BLOCK_N, "
"reduce_threads)) as bn:\n"
"            tn = T.get_thread_binding(0)\n"
"            tk = T.get_thread_binding(1)\n"
"            A_local = T.alloc_local((TILE_K,), dtype)\n"
"            B_local = T.alloc_local((TILE_K,), dtype)\n"
"            C_accum = T.alloc_local((1,), accum_dtype)\n"
"\n"
"            T.clear(C_accum)\n"
"            for bk in T.serial(T.ceildiv(K, BLOCK_K)):\n"
"                for k in T.vectorized(TILE_K):\n"
"                    A_local[k] = A[bk * BLOCK_K + tk * TILE_K + k]\n"
"                    B_local[k] = B[bn * BLOCK_N + tn, bk * BLOCK_K + tk * "
"TILE_K + k]\n"
"                for k in T.serial(TILE_K):\n"
"                    C_accum[0] += A_local[k].astype(accum_dtype) * "
"B_local[k].astype(accum_dtype)\n"
"            C_reduced = T.alloc_local((1,), accum_dtype)\n"
"            with T.attr(\n"
"                    T.comm_reducer(lambda x, y: x + y, [T.cast(0, "
"accum_dtype)]),\n"
"                    \"reduce_scope\",\n"
"                    T.reinterpret(T.uint64(0), dtype=\"handle\"),\n"
"            ):\n"
"                T.evaluate(\n"
"                    T.tvm_thread_allreduce(\n"
"                        T.uint32(1),\n"
"                        C_accum[0],\n"
"                        True,\n"
"                        C_reduced[0],\n"
"                        tk,\n"
"                        dtype=\"handle\",\n"
"                    ))\n"
"\n"
"            C[bn * BLOCK_N + tn] = C_reduced[0]\n"
"\n"
"    return main\n"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:314
msgid ""
"With this optimization, the kernel latency now reduces from **~0.0084 ms** "
"to **~0.0069 ms**, which is faster than torch/cuBLAS!"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:316
msgid "Autotune"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:318
msgid ""
"`BLOCK_N`, `BLOCK_K`, `reduce_threads` are hyperparameters in our kernel, "
"which can be tuned to improve performance. We can use the `tilelang."
"autotune` feature to automatically search for optimal configurations:"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:320
msgid ""
"def get_best_config(N, K):\n"
"\n"
"    def get_configs():\n"
"        BLOCK_N = [2, 4, 8, 32, 64, 128]\n"
"        reduce_threads = [4, 8, 32]\n"
"        _configs = list(itertools.product(\n"
"            BLOCK_N,\n"
"            reduce_threads,\n"
"        ))\n"
"        configs = [{\n"
"            \"BLOCK_N\": c[0],\n"
"            \"reduce_threads\": c[1],\n"
"        } for c in _configs]\n"
"        return configs\n"
"\n"
"    @autotune(\n"
"        configs=get_configs(),\n"
"        warmup=3,\n"
"        rep=20,\n"
"    )\n"
"    @jit(\n"
"        out_idx=[-1],\n"
"        supply_type=tl.TensorSupplyType.Integer,\n"
"        ref_prog=ref_program,\n"
"        skip_check=False,\n"
"        target=\"auto\",\n"
"    )\n"
"    def kernel(\n"
"        BLOCK_N=None,\n"
"        reduce_threads=None,\n"
"    ):\n"
"        dtype = \"float16\"\n"
"        accum_dtype = \"float\"\n"
"        MAX_TRANSACTION_SIZE_IN_BITS = 128\n"
"        TILE_K = MAX_TRANSACTION_SIZE_IN_BITS // DataType(dtype).bits\n"
"        BLOCK_K = reduce_threads * TILE_K\n"
"\n"
"        @T.prim_func\n"
"        def main(\n"
"                A: T.Buffer((K,), dtype),\n"
"                B: T.Buffer((N, K), dtype),\n"
"                C: T.Buffer((N,), dtype),\n"
"        ):\n"
"            with T.Kernel(T.ceildiv(N, BLOCK_N), threads=(BLOCK_N, "
"reduce_threads)) as bn:\n"
"                tn = T.get_thread_binding(0)\n"
"                tk = T.get_thread_binding(1)\n"
"                A_local = T.alloc_local((TILE_K,), dtype)\n"
"                B_local = T.alloc_local((TILE_K,), dtype)\n"
"                C_accum = T.alloc_local((1,), accum_dtype)\n"
"\n"
"                T.clear(C_accum)\n"
"                for bk in T.serial(T.ceildiv(K, BLOCK_K)):\n"
"                    for k in T.vectorized(TILE_K):\n"
"                        A_local[k] = A[bk * BLOCK_K + tk * TILE_K + k]\n"
"                        B_local[k] = B[bn * BLOCK_N + tn, bk * BLOCK_K + tk "
"* TILE_K + k]\n"
"                    for k in T.serial(TILE_K):\n"
"                        C_accum[0] += A_local[k].astype(accum_dtype) * "
"B_local[k].astype(accum_dtype)\n"
"                C_reduced = T.alloc_local((1,), accum_dtype)\n"
"                with T.attr(\n"
"                        T.comm_reducer(lambda x, y: x + y, [T.cast(0, "
"accum_dtype)]),\n"
"                        \"reduce_scope\",\n"
"                        T.reinterpret(T.uint64(0), dtype=\"handle\"),\n"
"                ):\n"
"                    T.evaluate(\n"
"                        T.tvm_thread_allreduce(\n"
"                            T.uint32(1),\n"
"                            C_accum[0],\n"
"                            True,\n"
"                            C_reduced[0],\n"
"                            tk,\n"
"                            dtype=\"handle\",\n"
"                        ))\n"
"\n"
"                C[bn * BLOCK_N + tn] = C_reduced[0]\n"
"\n"
"        return main\n"
"\n"
"    return kernel()\n"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:401
msgid ""
"After autotuning, now our kernel gets **~0.0067 ms**, the final generated "
"CUDA kernel might like this:"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:403
msgid ""
"extern \"C\" __global__ void __launch_bounds__(64, 1) main_kernel(half_t* "
"__restrict__ A, half_t* __restrict__ B, half_t* __restrict__ C) {\n"
"  float C_accum[1];\n"
"  half_t A_local[8];\n"
"  half_t B_local[8];\n"
"  __shared__ float red_buf0[64];\n"
"  C_accum[0] = 0.000000e+00f;\n"
"  for (int bk = 0; bk < 4; ++bk) {\n"
"    *(uint4*)(A_local + 0) = *(uint4*)(A + ((bk * 256) + (((int)threadIdx.y) "
"* 8)));\n"
"    *(uint4*)(B_local + 0) = *(uint4*)(B + ((((((int)blockIdx.x) * 2048) + "
"(((int)threadIdx.x) * 1024)) + (bk * 256)) + (((int)threadIdx.y) * 8)));\n"
"    for (int k = 0; k < 8; ++k) {\n"
"      C_accum[0] = (C_accum[0] + (((float)A_local[k]) * "
"((float)B_local[k])));\n"
"    }\n"
"  }\n"
"  tl::fence_proxy_async();\n"
"  __syncthreads();\n"
"  red_buf0[((((int)threadIdx.x) * 32) + ((int)threadIdx.y))] = C_accum[0];\n"
"  __syncthreads();\n"
"  if (((int)threadIdx.y) < 16) {\n"
"    red_buf0[((((int)threadIdx.x) * 32) + ((int)threadIdx.y))] = "
"(red_buf0[((((int)threadIdx.x) * 32) + ((int)threadIdx.y))] + "
"red_buf0[(((((int)threadIdx.x) * 32) + ((int)threadIdx.y)) + 16)]);\n"
"  }\n"
"  __syncthreads();\n"
"  if (((int)threadIdx.y) < 8) {\n"
"    red_buf0[((((int)threadIdx.x) * 32) + ((int)threadIdx.y))] = "
"(red_buf0[((((int)threadIdx.x) * 32) + ((int)threadIdx.y))] + "
"red_buf0[(((((int)threadIdx.x) * 32) + ((int)threadIdx.y)) + 8)]);\n"
"  }\n"
"  __syncthreads();\n"
"  if (((int)threadIdx.y) < 4) {\n"
"    red_buf0[((((int)threadIdx.x) * 32) + ((int)threadIdx.y))] = "
"(red_buf0[((((int)threadIdx.x) * 32) + ((int)threadIdx.y))] + "
"red_buf0[(((((int)threadIdx.x) * 32) + ((int)threadIdx.y)) + 4)]);\n"
"  }\n"
"  __syncthreads();\n"
"  if (((int)threadIdx.y) < 2) {\n"
"    red_buf0[((((int)threadIdx.x) * 32) + ((int)threadIdx.y))] = "
"(red_buf0[((((int)threadIdx.x) * 32) + ((int)threadIdx.y))] + "
"red_buf0[(((((int)threadIdx.x) * 32) + ((int)threadIdx.y)) + 2)]);\n"
"  }\n"
"  __syncthreads();\n"
"  if (((int)threadIdx.y) < 1) {\n"
"    red_buf0[((((int)threadIdx.x) * 32) + ((int)threadIdx.y))] = "
"(red_buf0[((((int)threadIdx.x) * 32) + ((int)threadIdx.y))] + "
"red_buf0[(((((int)threadIdx.x) * 32) + ((int)threadIdx.y)) + 1)]);\n"
"  }\n"
"  __syncthreads();\n"
"  C[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = "
"((half_t)red_buf0[(((int)threadIdx.x) * 32)]);\n"
"}\n"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:445
msgid ""
"This corresponds closely to our `TileLang` program, with necessary "
"synchronization and low-level optimizations inserted automatically."
msgstr ""

#: ../../../deeplearning_operators/gemv.md:447
msgid "Conclusion"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:449
msgid "Benchmark Table on Hopper GPU"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:13
msgid "Kernel Name"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:13
msgid "Latency"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:13
msgid "torch/cuBLAS"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:13
msgid "0.00784 ms"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:13
msgid "Triton"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:13
msgid "0.00773 ms"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:13
msgid "naive_gemv"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:13
msgid "0.16607 ms"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:13
msgid "splitk_gemv"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:13
msgid "0.02419 ms"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:13
msgid "splitk_gemv_vectorized"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:13
msgid "0.00809 ms"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:13
msgid "splitk_gemv_vectorized_tvm"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:13
msgid "0.00675 ms"
msgstr ""

#: ../../../deeplearning_operators/gemv.md:460
msgid ""
"Triton Time: 0.0077344514429569244 In this tutorial, we implemented a simple "
"GEMV kernel and learn that `TileLang` exposes low level control to user such "
"as thread-level programming and CUDA primitives."
msgstr ""
