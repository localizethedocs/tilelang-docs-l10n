# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025-2025, Tile Lang Contributors
# This file is distributed under the same license as the Tile Language <br> package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TileLang <br> stable\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-02-17 08:38+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: \n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../deeplearning_operators/elementwise.md:1
msgid "ElementWise Operators"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:3
msgid ""
"<div style=\"text-align: left;\">\n"
"    <em>Author:</em> <a href=\"https://github.com/chenghuaWang\">Chenghua "
"Wang</a>\n"
"</div>\n"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:11
msgid ""
"This document is still **experimental** and may be incomplete.    "
"Suggestions and improvements are highly encouraged—please submit a PR!"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:15
msgid ""
"Elementwise operators are widely used in deep learning and often serve as "
"the first example encountered by those beginning to explore parallel "
"programming. This tutorial will analyze several implementations of the "
"elementwise addition operator using TileLang and compare them with the "
"corresponding CUDA implementation. By the end of this tutorial, you will "
"learn:"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:17
msgid "How to implement an elementwise operator using TileLang."
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:18
msgid "How to compile operators with dynamic shapes."
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:19
#: ../../../deeplearning_operators/elementwise.md:98
msgid "How TileLang addresses boundary-related issues."
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:20
msgid ""
"The similarities and differences between operators implemented in TileLang "
"and those implemented in CUDA/CuTe."
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:22
msgid ""
"Please note that this tutorial does not delve deeply into the design "
"principles of TileLang. For a broader understanding of TileLang, we "
"recommend consulting the [Overview](../get_started/overview.md)."
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:24
msgid "Elementwise add in TileLang"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:26
msgid ""
"def elementwise_add(N, threads=256, dtype=T.bfloat16):\n"
"\n"
"    @T.prim_func\n"
"    def main(A: T.Tensor((N), dtype), B: T.Tensor((N), dtype), C: T."
"Tensor((N), dtype)):\n"
"        with T.Kernel(T.ceildiv(N, threads), threads=threads) as (b_x):\n"
"            # vector add.\n"
"            for i in T.Parallel(threads):\n"
"                C[b_x * threads + i] = A[b_x * threads + i] + B[b_x * "
"threads + i]\n"
"\n"
"    return main\n"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:39
msgid ""
"All logic for TileLang kernels must be implemented within the `T."
"Kernel(...)` scope. In this example, initializing `T.kernel(...)` requires "
"specifying both the grid size and the number of threads per block. The "
"returned value `bx` corresponds to `blockIdx.x` in CUDA. In the provided "
"implementation, `T.Parallel` is used to process the data tile (of size `1 x "
"threads`) assigned to the block for computation."
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:41
msgid ""
"Those familiar with CUDA programming might wonder where `threadIdx` fits "
"into this. Note that the code inside `T.Kernel` operates at the **block "
"level**, not the **thread level**. In this example, your focus is solely on "
"defining the block-level logic. During compilation, TileLang automatically "
"maps computations to the corresponding threads and applies further "
"optimizations. The optimized code generated by TileLang may closely align "
"with carefully handcrafted computational logic, as demonstrated in Section 2 "
"with a concrete example. While TileLang also supports thread-level "
"programming semantics, this will be covered in subsequent discussions."
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:43
msgid "The program can be compiled using the following code:"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:45
msgid ""
"program = elementwise_add(1024, threads=256, dtype=T.bfloat16)\n"
"kernel = tilelang.compile(program, out_idx=-1, target=\"cuda\", "
"execution_backend=\"cython\")\n"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:49
msgid ""
"Launching the kernel is straightforward, just call it directly like a "
"function:"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:51
msgid "C = kernel(A, B)\n"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:55
msgid ""
"The vector add operation can also be extended to two-dimensional cases, "
"where both implementations demonstrate comparable efficiency in practice. "
"Below is an example from the test section that readers can refer to: "
"[example](https://github.com/tile-ai/tilelang/blob/main/testing/python/"
"kernel/test_tilelang_kernel_element_wise_add.py). The code for this kernel "
"is provided below:"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:57
msgid ""
"import tilelang.language as T\n"
"def elementwise_add(\n"
"    M,\n"
"    N,\n"
"    block_M,\n"
"    block_N,\n"
"    in_dtype,\n"
"    out_dtype,\n"
"    threads,\n"
"):\n"
"    @T.prim_func\n"
"    def main(\n"
"            A: T.Tensor((M, N), in_dtype),\n"
"            B: T.Tensor((M, N), in_dtype),\n"
"            C: T.Tensor((M, N), out_dtype),\n"
"    ):\n"
"        with T.Kernel(T.ceildiv(N, block_N), T.ceildiv(M, block_M), "
"threads=threads) as (bx, by):\n"
"            start_x = bx * block_N\n"
"            start_y = by * block_M\n"
"\n"
"            for (local_y, local_x) in T.Parallel(block_M, block_N):\n"
"                y = start_y + local_y\n"
"                x = start_x + local_x\n"
"\n"
"                C[y, x] = A[y, x] + B[y, x]\n"
"\n"
"    return main\n"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:87
msgid "How to compile operators with dynamic shapes?"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:89
msgid ""
"In the compilation process above, a fixed shape was used. However, in "
"practical usage, we often want the kernel to support dynamic shapes. So, how "
"can we compile a kernel in TileLang to handle dynamic shapes? In TileLang, "
"we can replace the target size with a dynamic symbolic value, making the "
"dimension dynamic. The following example illustrates this:"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:91
msgid ""
"program = elementwise_add(T.dynamic(\"N\"), threads=256, dtype=T.bfloat16)\n"
"kernel = tilelang.compile(program, out_idx=-1, target=\"cuda\", "
"execution_backend=\"cython\")\n"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:96
msgid ""
"The resulting CUDA code for the kernel will include an additional `int N` "
"parameter after the `bfloat16_t* __restrict__ A`, `bfloat16_t* __restrict__ "
"B`, and `bfloat16_t* __restrict__ C` parameters."
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:100
msgid ""
"TileLang automatically incorporates boundary-checking conditions; however, "
"this comes at a cost. These boundary conditions may prevent TileLang from "
"performing more advanced optimizations. I will introduce an example from the "
"next section in advance. The corresponding code is also provided below, but "
"note that it involves the associated CUDA code. Readers are encouraged to "
"first review the next section before returning to this paragraph for a "
"clearer understanding."
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:102
msgid "When compiling the example below, let's set `N` to 2047:"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:104
#: ../../../deeplearning_operators/elementwise.md:178
msgid ""
"def elementwise_add(N, num_per_thread=8, threads=256, dtype=T.bfloat16):\n"
"\n"
"    @T.prim_func\n"
"    def main(A: T.Tensor((N), dtype), B: T.Tensor((N), dtype), C: T."
"Tensor((N), dtype)):\n"
"        with T.Kernel(T.ceildiv(N, threads * num_per_thread), "
"threads=threads) as (b_x):\n"
"            # vector add.\n"
"            for i, j in T.Parallel(threads, num_per_thread):\n"
"                offsets = (b_x * threads + i) * num_per_thread\n"
"                C[offsets + j] = A[offsets + j] + B[offsets + j]\n"
"\n"
"    return main\n"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:118
msgid "TileLang will generate the following CUDA code:"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:120
msgid ""
"extern \"C\" __global__ void __launch_bounds__(256) main_kernel(bfloat16_t* "
"__restrict__ A, bfloat16_t* __restrict__ B, bfloat16_t* __restrict__ C) {\n"
"  #pragma unroll\n"
"  for (int i = 0; i < 8; ++i) {\n"
"    if (((i * 256) + ((int)threadIdx.x)) < 2047) {\n"
"      C[((i * 256) + ((int)threadIdx.x))] = (A[((i * 256) + ((int)threadIdx."
"x))] + B[((i * 256) + ((int)threadIdx.x))]);\n"
"    }\n"
"  }\n"
"}\n"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:131
msgid ""
"We can observe that TileLang did not apply optimizations such as "
"vectorization or coalesced memory access. In fact, except for the tail group "
"of data, all other threads could have executed more optimized code."
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:133
msgid "Comparison of TileLang, CUDA, and CuTe"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:135
msgid ""
"For the subsequent examples, this tutorial will use the vector add operation "
"for simplicity and brevity."
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:137
msgid ""
"Typically, those new to CUDA programming often write CUDA code in a style "
"similar to this:"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:139
msgid ""
"// vector add\n"
"__global__ void elementwise_add(float* a, float* b, float* c, int N) {\n"
"    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n"
"    if (idx < N) {\n"
"        c[idx] = a[idx] + b[idx];\n"
"    }\n"
"}\n"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:149
msgid ""
"The code above assigns each thread to compute a single element, which is "
"evidently inefficient since common acceleration techniques like coalesced "
"memory access and vectorization are not utilized. However, TileLang code "
"written with similar logic (e.g., loop-based traversal) can be optimized by "
"the compiler into highly efficient implementations, making it more "
"accessible for beginners. Additionally, the final generated code from the "
"compiler remains observable, providing transparency into the optimization "
"process."
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:151
msgid ""
"The CUDA code generated by TileLang for the compiled kernel can be retrieved "
"using the `kernel.get_kernel_source()` method. Below is the CUDA code "
"produced for the vector addition example from Section 1:"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:153
msgid ""
"extern \"C\" __global__ void __launch_bounds__(256) main_kernel(bfloat16_t* "
"__restrict__ A, bfloat16_t* __restrict__ B, bfloat16_t* __restrict__ C) {\n"
"  if (((int)threadIdx.x) < 32) {\n"
"    uint4 __1;\n"
"      uint4 v_ = *(uint4*)(A + ((((int)blockIdx.x) * 256) + (((int)threadIdx."
"x) * 8)));\n"
"      uint4 v__1 = *(uint4*)(B + ((((int)blockIdx.x) * 256) + "
"(((int)threadIdx.x) * 8)));\n"
"      ((nv_bfloat162*)(&(__1.x)))->x = (((nv_bfloat162*)(&(v_.x)))-"
">x+((nv_bfloat162*)(&(v__1.x)))->x);\n"
"      ((nv_bfloat162*)(&(__1.x)))->y = (((nv_bfloat162*)(&(v_.x)))-"
">y+((nv_bfloat162*)(&(v__1.x)))->y);\n"
"      ((nv_bfloat162*)(&(__1.y)))->x = (((nv_bfloat162*)(&(v_.y)))-"
">x+((nv_bfloat162*)(&(v__1.y)))->x);\n"
"      ((nv_bfloat162*)(&(__1.y)))->y = (((nv_bfloat162*)(&(v_.y)))-"
">y+((nv_bfloat162*)(&(v__1.y)))->y);\n"
"      ((nv_bfloat162*)(&(__1.z)))->x = (((nv_bfloat162*)(&(v_.z)))-"
">x+((nv_bfloat162*)(&(v__1.z)))->x);\n"
"      ((nv_bfloat162*)(&(__1.z)))->y = (((nv_bfloat162*)(&(v_.z)))-"
">y+((nv_bfloat162*)(&(v__1.z)))->y);\n"
"      ((nv_bfloat162*)(&(__1.w)))->x = (((nv_bfloat162*)(&(v_.w)))-"
">x+((nv_bfloat162*)(&(v__1.w)))->x);\n"
"      ((nv_bfloat162*)(&(__1.w)))->y = (((nv_bfloat162*)(&(v_.w)))-"
">y+((nv_bfloat162*)(&(v__1.w)))->y);\n"
"    *(uint4*)(C + ((((int)blockIdx.x) * 256) + (((int)threadIdx.x) * 8))) = "
"__1;\n"
"  }\n"
"}\n"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:172
msgid ""
"In the code above, TileLang not only automatically maps block-level "
"parallelism to threads but also applies optimizations such as vectorization "
"and coalesced memory access."
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:174
msgid ""
"While TileLang incorporates various optimizations for the aforementioned "
"case, its behavior may sometimes appear counterintuitive. For example, when "
"targeting 256 threads for task processing, applying vectorization can result "
"in each thread computing 8 data elements—effectively utilizing only 32 "
"active threads. Interestingly, the kernel launch configuration still retains "
"the original allocation of 256 threads."
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:176
msgid ""
"In such scenarios, explicitly specifying the number of elements computed per "
"thread can help \"guide\" TileLang's code generation process, leading to "
"implementations that are more closely aligned with the intended design."
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:192
msgid ""
"The corresponding CUDA code generated for the above example is presented "
"below:"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:194
msgid ""
"extern \"C\" __global__ void __launch_bounds__(256) main_kernel(bfloat16_t* "
"__restrict__ A, bfloat16_t* __restrict__ B, bfloat16_t* __restrict__ C) {\n"
"  uint4 __1;\n"
"    uint4 v_ = *(uint4*)(A + (((int)threadIdx.x) * 8));\n"
"    uint4 v__1 = *(uint4*)(B + (((int)threadIdx.x) * 8));\n"
"    ((nv_bfloat162*)(&(__1.x)))->x = (((nv_bfloat162*)(&(v_.x)))-"
">x+((nv_bfloat162*)(&(v__1.x)))->x);\n"
"    ((nv_bfloat162*)(&(__1.x)))->y = (((nv_bfloat162*)(&(v_.x)))-"
">y+((nv_bfloat162*)(&(v__1.x)))->y);\n"
"    ((nv_bfloat162*)(&(__1.y)))->x = (((nv_bfloat162*)(&(v_.y)))-"
">x+((nv_bfloat162*)(&(v__1.y)))->x);\n"
"    ((nv_bfloat162*)(&(__1.y)))->y = (((nv_bfloat162*)(&(v_.y)))-"
">y+((nv_bfloat162*)(&(v__1.y)))->y);\n"
"    ((nv_bfloat162*)(&(__1.z)))->x = (((nv_bfloat162*)(&(v_.z)))-"
">x+((nv_bfloat162*)(&(v__1.z)))->x);\n"
"    ((nv_bfloat162*)(&(__1.z)))->y = (((nv_bfloat162*)(&(v_.z)))-"
">y+((nv_bfloat162*)(&(v__1.z)))->y);\n"
"    ((nv_bfloat162*)(&(__1.w)))->x = (((nv_bfloat162*)(&(v_.w)))-"
">x+((nv_bfloat162*)(&(v__1.w)))->x);\n"
"    ((nv_bfloat162*)(&(__1.w)))->y = (((nv_bfloat162*)(&(v_.w)))-"
">y+((nv_bfloat162*)(&(v__1.w)))->y);\n"
"  *(uint4*)(C + (((int)threadIdx.x) * 8)) = __1;\n"
"}\n"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:210
msgid ""
"Aha, this CUDA code aligns closely with conventional programming practices, "
"making it more familiar and intuitive."
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:212
msgid ""
"But what happens if we provide additional hints to TileLang? For instance, "
"by explicitly specifying register copies using the `T.copy(...)` operation. "
"The example below demonstrates a vector addition implementation. Unlike the "
"previous examples, this code explicitly loads data into registers before "
"performing computations."
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:214
msgid ""
"def elementwise_add(N, NUM_ELE_PER_THREAD=8, threads=256, dtype=T."
"bfloat16):\n"
"\n"
"    @T.prim_func\n"
"    def main(A: T.Tensor((N), dtype), B: T.Tensor((N), dtype), C: T."
"Tensor((N), dtype)):\n"
"        with T.Kernel(T.ceildiv(N, threads * NUM_ELE_PER_THREAD), "
"threads=threads) as (b_x):\n"
"            A_register = T.alloc_fragment((threads * NUM_ELE_PER_THREAD), "
"dtype)\n"
"            B_register = T.alloc_fragment((threads * NUM_ELE_PER_THREAD), "
"dtype)\n"
"            C_register = T.alloc_fragment((threads * NUM_ELE_PER_THREAD), "
"dtype)\n"
"\n"
"            s_start = b_x * threads * NUM_ELE_PER_THREAD\n"
"            s_end = (b_x + 1) * threads * NUM_ELE_PER_THREAD\n"
"\n"
"            # LDG. 128\n"
"            T.copy(\n"
"                A[s_start:s_end],\n"
"                A_register,\n"
"            )\n"
"            T.copy(\n"
"                B[s_start:s_end],\n"
"                B_register,\n"
"            )\n"
"\n"
"            # vector add.\n"
"            for tid, i in T.Parallel(threads, NUM_ELE_PER_THREAD):\n"
"                C_register[tid * NUM_ELE_PER_THREAD + i] = (\n"
"                    A_register[tid * NUM_ELE_PER_THREAD + i] +\n"
"                    B_register[tid * NUM_ELE_PER_THREAD + i])\n"
"\n"
"            # STG. 128\n"
"            T.copy(\n"
"                C_register,\n"
"                C[s_start:s_end],\n"
"            )\n"
"\n"
"    return main\n"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:252
msgid ""
"In the example above, each thread is responsible for computing 8 elements. "
"The `T.copy(...)` method functions at the block level, and TileLang "
"automatically maps data movement operations to individual threads. This "
"design may resonate more intuitively with CUDA developers. Let us now "
"analyze the CUDA code generated from this implementation."
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:254
msgid ""
"// N is set to 8192 * 8192 when compiling\n"
"extern \"C\" __global__ void __launch_bounds__(256) main_kernel(bfloat16_t* "
"__restrict__ A, bfloat16_t* __restrict__ B, bfloat16_t* __restrict__ C) {\n"
"  bfloat16_t A_register[8];\n"
"  bfloat16_t B_register[8];\n"
"  *(uint4*)(A_register + 0) = *(uint4*)(A + ((((int)blockIdx.x) * 2048) + "
"(((int)threadIdx.x) * 8)));\n"
"  *(uint4*)(B_register + 0) = *(uint4*)(B + ((((int)blockIdx.x) * 2048) + "
"(((int)threadIdx.x) * 8)));\n"
"  uint4 __1;\n"
"    uint4 v_ = *(uint4*)(A_register + 0);\n"
"    uint4 v__1 = *(uint4*)(B_register + 0);\n"
"    ((nv_bfloat162*)(&(__1.x)))->x = (((nv_bfloat162*)(&(v_.x)))-"
">x+((nv_bfloat162*)(&(v__1.x)))->x);\n"
"    ((nv_bfloat162*)(&(__1.x)))->y = (((nv_bfloat162*)(&(v_.x)))-"
">y+((nv_bfloat162*)(&(v__1.x)))->y);\n"
"    ((nv_bfloat162*)(&(__1.y)))->x = (((nv_bfloat162*)(&(v_.y)))-"
">x+((nv_bfloat162*)(&(v__1.y)))->x);\n"
"    ((nv_bfloat162*)(&(__1.y)))->y = (((nv_bfloat162*)(&(v_.y)))-"
">y+((nv_bfloat162*)(&(v__1.y)))->y);\n"
"    ((nv_bfloat162*)(&(__1.z)))->x = (((nv_bfloat162*)(&(v_.z)))-"
">x+((nv_bfloat162*)(&(v__1.z)))->x);\n"
"    ((nv_bfloat162*)(&(__1.z)))->y = (((nv_bfloat162*)(&(v_.z)))-"
">y+((nv_bfloat162*)(&(v__1.z)))->y);\n"
"    ((nv_bfloat162*)(&(__1.w)))->x = (((nv_bfloat162*)(&(v_.w)))-"
">x+((nv_bfloat162*)(&(v__1.w)))->x);\n"
"    ((nv_bfloat162*)(&(__1.w)))->y = (((nv_bfloat162*)(&(v_.w)))-"
">y+((nv_bfloat162*)(&(v__1.w)))->y);\n"
"  *(uint4*)(A_register + 0) = __1;\n"
"  *(uint4*)(C + ((((int)blockIdx.x) * 2048) + (((int)threadIdx.x) * 8))) = "
"*(uint4*)(A_register + 0);\n"
"}\n"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:277
msgid ""
"We observed the emergence of two additional registers, `A_register` and "
"`B_register`. However, during the actual computation, these registers are "
"simply reassigned to `v_` and `v__1`, respectively."
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:279
msgid ""
"To evaluate complexity, one could implement the same elementwise addition "
"operator using CuTe and compare it with the TileLang version. The "
"corresponding CuTe code is provided below:"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:281
msgid ""
"template<int NUM_ELE_PER_THREAD=8>\n"
"__global__ void elementwise_add(nv_bfloat16* C,\n"
"                                 const nv_bfloat16* A,\n"
"                                 const nv_bfloat16* B,\n"
"                                 int N) {\n"
"  using namespace cute;\n"
"\n"
"  const int idx = threadIdx.x + blockIdx.x * blockDim.x;\n"
"\n"
"  Tensor t_C = make_tensor(make_gmem_ptr(C), make_shape(N));\n"
"  Tensor t_A = make_tensor(make_gmem_ptr(A), make_shape(N));\n"
"  Tensor t_B = make_tensor(make_gmem_ptr(B), make_shape(N));\n"
"\n"
"  Tensor t_C_tile = local_tile(t_C, make_shape(Int<NUM_ELE_PER_THREAD>{}), "
"make_coord(idx));\n"
"  Tensor t_A_tile = local_tile(t_A, make_shape(Int<NUM_ELE_PER_THREAD>{}), "
"make_coord(idx));\n"
"  Tensor t_B_tile = local_tile(t_B, make_shape(Int<NUM_ELE_PER_THREAD>{}), "
"make_coord(idx));\n"
"\n"
"  Tensor reg_buffer_A = make_tensor_like(t_A_tile);\n"
"  Tensor reg_buffer_B = make_tensor_like(t_B_tile);\n"
"  Tensor reg_buffer_C = make_tensor_like(t_C_tile);\n"
"\n"
"  // LDG. 128\n"
"  copy(t_A_tile, reg_buffer_A);\n"
"  copy(t_B_tile, reg_buffer_B);\n"
"\n"
"  auto reg_C_vector = recast<nv_bfloat162>(reg_buffer_C);\n"
"  auto reg_A_vector = recast<nv_bfloat162>(reg_buffer_A);\n"
"  auto reg_B_vector = recast<nv_bfloat162>(reg_buffer_B);\n"
"\n"
"  // Perform vectorized addition\n"
"#pragma unroll\n"
"  for (int vec_idx = 0; vec_idx < size(reg_C_vector); ++vec_idx) {\n"
"    reg_C_vector(vec_idx) = reg_A_vector(vec_idx) + reg_B_vector(vec_idx);\n"
"  }\n"
"\n"
"  auto reg_C_flat = recast<nv_bfloat16>(reg_C_vector);\n"
"\n"
"  // STG. 128\n"
"  copy(reg_C_flat, t_C_tile);\n"
"}\n"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:324
msgid "Conclusion"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:326
msgid ""
"This tutorial showcases the implementation of the elementwise addition "
"operator using TileLang, while also comparing various design approaches. "
"TileLang significantly reduces the complexity of CUDA programming, enabling "
"high performance with minimal code. Nevertheless, working with TileLang "
"demands careful attention to specific implementation details. To ensure "
"computational efficiency, it is essential to thoroughly examine the "
"generated CUDA kernels."
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:330
msgid "**Reference:**"
msgstr ""

#: ../../../deeplearning_operators/elementwise.md:332
msgid ""
"[1] The CuTe code implementation draws inspiration from the techniques "
"discussed in this blog: https://zhuanlan.zhihu.com/p/690703999"
msgstr ""
